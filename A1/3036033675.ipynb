{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wI4A-T0z5AU6"
   },
   "source": [
    "# Assignment 1\n",
    "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEomoMzH5Nf6"
   },
   "source": [
    "## 1 $n$-gram Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "YsSAtTqt7Q8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-10-10 22:37:51--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/train.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 11185077 (11M) [text/plain]\n",
      "Saving to: 'train.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  0% 1.39M 8s\n",
      "    50K .......... .......... .......... .......... ..........  0% 58.2M 4s\n",
      "   100K .......... .......... .......... .......... ..........  1% 27.9M 3s\n",
      "   150K .......... .......... .......... .......... ..........  1% 9.45M 2s\n",
      "   200K .......... .......... .......... .......... ..........  2% 11.1M 2s\n",
      "   250K .......... .......... .......... .......... ..........  2% 7.73M 2s\n",
      "   300K .......... .......... .......... .......... ..........  3% 12.7M 2s\n",
      "   350K .......... .......... .......... .......... ..........  3% 6.09M 2s\n",
      "   400K .......... .......... .......... .......... ..........  4% 10.1M 2s\n",
      "   450K .......... .......... .......... .......... ..........  4% 18.2M 2s\n",
      "   500K .......... .......... .......... .......... ..........  5% 14.5M 1s\n",
      "   550K .......... .......... .......... .......... ..........  5% 17.2M 1s\n",
      "   600K .......... .......... .......... .......... ..........  5% 18.9M 1s\n",
      "   650K .......... .......... .......... .......... ..........  6% 12.7M 1s\n",
      "   700K .......... .......... .......... .......... ..........  6% 10.4M 1s\n",
      "   750K .......... .......... .......... .......... ..........  7% 14.6M 1s\n",
      "   800K .......... .......... .......... .......... ..........  7% 5.97M 1s\n",
      "   850K .......... .......... .......... .......... ..........  8% 27.9M 1s\n",
      "   900K .......... .......... .......... .......... ..........  8% 13.8M 1s\n",
      "   950K .......... .......... .......... .......... ..........  9% 8.75M 1s\n",
      "  1000K .......... .......... .......... .......... ..........  9% 24.3M 1s\n",
      "  1050K .......... .......... .......... .......... .......... 10% 8.96M 1s\n",
      "  1100K .......... .......... .......... .......... .......... 10% 53.7M 1s\n",
      "  1150K .......... .......... .......... .......... .......... 10% 10.8M 1s\n",
      "  1200K .......... .......... .......... .......... .......... 11% 25.6M 1s\n",
      "  1250K .......... .......... .......... .......... .......... 11% 12.6M 1s\n",
      "  1300K .......... .......... .......... .......... .......... 12% 15.7M 1s\n",
      "  1350K .......... .......... .......... .......... .......... 12% 18.7M 1s\n",
      "  1400K .......... .......... .......... .......... .......... 13% 11.4M 1s\n",
      "  1450K .......... .......... .......... .......... .......... 13% 21.7M 1s\n",
      "  1500K .......... .......... .......... .......... .......... 14% 12.6M 1s\n",
      "  1550K .......... .......... .......... .......... .......... 14% 6.34M 1s\n",
      "  1600K .......... .......... .......... .......... .......... 15% 14.9M 1s\n",
      "  1650K .......... .......... .......... .......... .......... 15% 27.2M 1s\n",
      "  1700K .......... .......... .......... .......... .......... 16% 9.45M 1s\n",
      "  1750K .......... .......... .......... .......... .......... 16% 10.7M 1s\n",
      "  1800K .......... .......... .......... .......... .......... 16% 26.1M 1s\n",
      "  1850K .......... .......... .......... .......... .......... 17% 17.6M 1s\n",
      "  1900K .......... .......... .......... .......... .......... 17% 14.9M 1s\n",
      "  1950K .......... .......... .......... .......... .......... 18% 12.7M 1s\n",
      "  2000K .......... .......... .......... .......... .......... 18% 17.3M 1s\n",
      "  2050K .......... .......... .......... .......... .......... 19% 17.6M 1s\n",
      "  2100K .......... .......... .......... .......... .......... 19% 20.1M 1s\n",
      "  2150K .......... .......... .......... .......... .......... 20% 10.6M 1s\n",
      "  2200K .......... .......... .......... .......... .......... 20% 18.9M 1s\n",
      "  2250K .......... .......... .......... .......... .......... 21% 6.07M 1s\n",
      "  2300K .......... .......... .......... .......... .......... 21% 30.7M 1s\n",
      "  2350K .......... .......... .......... .......... .......... 21% 12.0M 1s\n",
      "  2400K .......... .......... .......... .......... .......... 22% 9.70M 1s\n",
      "  2450K .......... .......... .......... .......... .......... 22% 54.4M 1s\n",
      "  2500K .......... .......... .......... .......... .......... 23% 9.32M 1s\n",
      "  2550K .......... .......... .......... .......... .......... 23% 18.7M 1s\n",
      "  2600K .......... .......... .......... .......... .......... 24% 18.2M 1s\n",
      "  2650K .......... .......... .......... .......... .......... 24% 11.4M 1s\n",
      "  2700K .......... .......... .......... .......... .......... 25% 17.3M 1s\n",
      "  2750K .......... .......... .......... .......... .......... 25% 10.2M 1s\n",
      "  2800K .......... .......... .......... .......... .......... 26% 23.4M 1s\n",
      "  2850K .......... .......... .......... .......... .......... 26% 25.3M 1s\n",
      "  2900K .......... .......... .......... .......... .......... 27% 14.8M 1s\n",
      "  2950K .......... .......... .......... .......... .......... 27% 23.3M 1s\n",
      "  3000K .......... .......... .......... .......... .......... 27% 6.47M 1s\n",
      "  3050K .......... .......... .......... .......... .......... 28% 22.4M 1s\n",
      "  3100K .......... .......... .......... .......... .......... 28% 25.9M 1s\n",
      "  3150K .......... .......... .......... .......... .......... 29% 11.9M 1s\n",
      "  3200K .......... .......... .......... .......... .......... 29% 21.6M 1s\n",
      "  3250K .......... .......... .......... .......... .......... 30% 11.5M 1s\n",
      "  3300K .......... .......... .......... .......... .......... 30% 9.51M 1s\n",
      "  3350K .......... .......... .......... .......... .......... 31% 27.6M 1s\n",
      "  3400K .......... .......... .......... .......... .......... 31% 23.8M 1s\n",
      "  3450K .......... .......... .......... .......... .......... 32% 6.60M 1s\n",
      "  3500K .......... .......... .......... .......... .......... 32% 27.9M 1s\n",
      "  3550K .......... .......... .......... .......... .......... 32% 11.0M 1s\n",
      "  3600K .......... .......... .......... .......... .......... 33% 35.4M 1s\n",
      "  3650K .......... .......... .......... .......... .......... 33% 9.53M 1s\n",
      "  3700K .......... .......... .......... .......... .......... 34% 13.4M 1s\n",
      "  3750K .......... .......... .......... .......... .......... 34% 5.43M 1s\n",
      "  3800K .......... .......... .......... .......... .......... 35% 16.6M 1s\n",
      "  3850K .......... .......... .......... .......... .......... 35% 22.6M 1s\n",
      "  3900K .......... .......... .......... .......... .......... 36% 8.47M 1s\n",
      "  3950K .......... .......... .......... .......... .......... 36% 5.99M 1s\n",
      "  4000K .......... .......... .......... .......... .......... 37% 12.9M 1s\n",
      "  4050K .......... .......... .......... .......... .......... 37% 11.9M 1s\n",
      "  4100K .......... .......... .......... .......... .......... 37% 5.25M 1s\n",
      "  4150K .......... .......... .......... .......... .......... 38% 8.53M 1s\n",
      "  4200K .......... .......... .......... .......... .......... 38% 8.85M 1s\n",
      "  4250K .......... .......... .......... .......... .......... 39% 10.2M 1s\n",
      "  4300K .......... .......... .......... .......... .......... 39% 20.7M 1s\n",
      "  4350K .......... .......... .......... .......... .......... 40% 10.6M 1s\n",
      "  4400K .......... .......... .......... .......... .......... 40% 15.7M 1s\n",
      "  4450K .......... .......... .......... .......... .......... 41% 21.3M 1s\n",
      "  4500K .......... .......... .......... .......... .......... 41% 30.4M 1s\n",
      "  4550K .......... .......... .......... .......... .......... 42% 9.01M 1s\n",
      "  4600K .......... .......... .......... .......... .......... 42% 23.4M 1s\n",
      "  4650K .......... .......... .......... .......... .......... 43% 14.1M 1s\n",
      "  4700K .......... .......... .......... .......... .......... 43% 18.4M 1s\n",
      "  4750K .......... .......... .......... .......... .......... 43% 18.7M 1s\n",
      "  4800K .......... .......... .......... .......... .......... 44% 21.1M 0s\n",
      "  4850K .......... .......... .......... .......... .......... 44% 5.81M 0s\n",
      "  4900K .......... .......... .......... .......... .......... 45% 7.49M 0s\n",
      "  4950K .......... .......... .......... .......... .......... 45% 21.3M 0s\n",
      "  5000K .......... .......... .......... .......... .......... 46% 31.2M 0s\n",
      "  5050K .......... .......... .......... .......... .......... 46% 9.92M 0s\n",
      "  5100K .......... .......... .......... .......... .......... 47% 13.2M 0s\n",
      "  5150K .......... .......... .......... .......... .......... 47% 14.5M 0s\n",
      "  5200K .......... .......... .......... .......... .......... 48% 11.6M 0s\n",
      "  5250K .......... .......... .......... .......... .......... 48% 42.8M 0s\n",
      "  5300K .......... .......... .......... .......... .......... 48% 18.6M 0s\n",
      "  5350K .......... .......... .......... .......... .......... 49% 19.7M 0s\n",
      "  5400K .......... .......... .......... .......... .......... 49% 11.9M 0s\n",
      "  5450K .......... .......... .......... .......... .......... 50% 21.2M 0s\n",
      "  5500K .......... .......... .......... .......... .......... 50% 18.7M 0s\n",
      "  5550K .......... .......... .......... .......... .......... 51% 21.1M 0s\n",
      "  5600K .......... .......... .......... .......... .......... 51% 9.94M 0s\n",
      "  5650K .......... .......... .......... .......... .......... 52% 21.6M 0s\n",
      "  5700K .......... .......... .......... .......... .......... 52% 8.75M 0s\n",
      "  5750K .......... .......... .......... .......... .......... 53% 14.0M 0s\n",
      "  5800K .......... .......... .......... .......... .......... 53% 24.1M 0s\n",
      "  5850K .......... .......... .......... .......... .......... 54% 10.3M 0s\n",
      "  5900K .......... .......... .......... .......... .......... 54% 14.6M 0s\n",
      "  5950K .......... .......... .......... .......... .......... 54% 17.1M 0s\n",
      "  6000K .......... .......... .......... .......... .......... 55% 14.0M 0s\n",
      "  6050K .......... .......... .......... .......... .......... 55% 11.8M 0s\n",
      "  6100K .......... .......... .......... .......... .......... 56% 13.2M 0s\n",
      "  6150K .......... .......... .......... .......... .......... 56% 12.3M 0s\n",
      "  6200K .......... .......... .......... .......... .......... 57% 17.2M 0s\n",
      "  6250K .......... .......... .......... .......... .......... 57% 10.4M 0s\n",
      "  6300K .......... .......... .......... .......... .......... 58% 7.44M 0s\n",
      "  6350K .......... .......... .......... .......... .......... 58% 6.04M 0s\n",
      "  6400K .......... .......... .......... .......... .......... 59% 16.1M 0s\n",
      "  6450K .......... .......... .......... .......... .......... 59% 15.2M 0s\n",
      "  6500K .......... .......... .......... .......... .......... 59% 17.9M 0s\n",
      "  6550K .......... .......... .......... .......... .......... 60% 13.8M 0s\n",
      "  6600K .......... .......... .......... .......... .......... 60% 8.80M 0s\n",
      "  6650K .......... .......... .......... .......... .......... 61% 19.7M 0s\n",
      "  6700K .......... .......... .......... .......... .......... 61% 18.1M 0s\n",
      "  6750K .......... .......... .......... .......... .......... 62% 13.1M 0s\n",
      "  6800K .......... .......... .......... .......... .......... 62% 12.6M 0s\n",
      "  6850K .......... .......... .......... .......... .......... 63% 10.9M 0s\n",
      "  6900K .......... .......... .......... .......... .......... 63% 7.81M 0s\n",
      "  6950K .......... .......... .......... .......... .......... 64%  103M 0s\n",
      "  7000K .......... .......... .......... .......... .......... 64% 7.40M 0s\n",
      "  7050K .......... .......... .......... .......... .......... 65% 41.8M 0s\n",
      "  7100K .......... .......... .......... .......... .......... 65% 10.8M 0s\n",
      "  7150K .......... .......... .......... .......... .......... 65% 12.8M 0s\n",
      "  7200K .......... .......... .......... .......... .......... 66% 22.9M 0s\n",
      "  7250K .......... .......... .......... .......... .......... 66% 22.8M 0s\n",
      "  7300K .......... .......... .......... .......... .......... 67% 16.2M 0s\n",
      "  7350K .......... .......... .......... .......... .......... 67% 18.0M 0s\n",
      "  7400K .......... .......... .......... .......... .......... 68% 21.4M 0s\n",
      "  7450K .......... .......... .......... .......... .......... 68% 13.8M 0s\n",
      "  7500K .......... .......... .......... .......... .......... 69% 21.0M 0s\n",
      "  7550K .......... .......... .......... .......... .......... 69% 20.5M 0s\n",
      "  7600K .......... .......... .......... .......... .......... 70% 16.4M 0s\n",
      "  7650K .......... .......... .......... .......... .......... 70% 6.56M 0s\n",
      "  7700K .......... .......... .......... .......... .......... 70% 9.95M 0s\n",
      "  7750K .......... .......... .......... .......... .......... 71% 8.53M 0s\n",
      "  7800K .......... .......... .......... .......... .......... 71% 17.5M 0s\n",
      "  7850K .......... .......... .......... .......... .......... 72% 10.7M 0s\n",
      "  7900K .......... .......... .......... .......... .......... 72% 13.6M 0s\n",
      "  7950K .......... .......... .......... .......... .......... 73% 9.15M 0s\n",
      "  8000K .......... .......... .......... .......... .......... 73% 21.6M 0s\n",
      "  8050K .......... .......... .......... .......... .......... 74% 27.3M 0s\n",
      "  8100K .......... .......... .......... .......... .......... 74% 14.2M 0s\n",
      "  8150K .......... .......... .......... .......... .......... 75% 17.4M 0s\n",
      "  8200K .......... .......... .......... .......... .......... 75% 8.88M 0s\n",
      "  8250K .......... .......... .......... .......... .......... 75% 8.75M 0s\n",
      "  8300K .......... .......... .......... .......... .......... 76% 31.6M 0s\n",
      "  8350K .......... .......... .......... .......... .......... 76% 15.8M 0s\n",
      "  8400K .......... .......... .......... .......... .......... 77% 15.2M 0s\n",
      "  8450K .......... .......... .......... .......... .......... 77% 7.89M 0s\n",
      "  8500K .......... .......... .......... .......... .......... 78% 17.6M 0s\n",
      "  8550K .......... .......... .......... .......... .......... 78% 10.9M 0s\n",
      "  8600K .......... .......... .......... .......... .......... 79% 19.9M 0s\n",
      "  8650K .......... .......... .......... .......... .......... 79% 32.1M 0s\n",
      "  8700K .......... .......... .......... .......... .......... 80% 11.6M 0s\n",
      "  8750K .......... .......... .......... .......... .......... 80% 14.0M 0s\n",
      "  8800K .......... .......... .......... .......... .......... 81% 21.4M 0s\n",
      "  8850K .......... .......... .......... .......... .......... 81% 27.8M 0s\n",
      "  8900K .......... .......... .......... .......... .......... 81% 10.2M 0s\n",
      "  8950K .......... .......... .......... .......... .......... 82% 14.4M 0s\n",
      "  9000K .......... .......... .......... .......... .......... 82% 17.9M 0s\n",
      "  9050K .......... .......... .......... .......... .......... 83% 25.5M 0s\n",
      "  9100K .......... .......... .......... .......... .......... 83% 9.97M 0s\n",
      "  9150K .......... .......... .......... .......... .......... 84% 7.00M 0s\n",
      "  9200K .......... .......... .......... .......... .......... 84% 11.1M 0s\n",
      "  9250K .......... .......... .......... .......... .......... 85% 8.58M 0s\n",
      "  9300K .......... .......... .......... .......... .......... 85% 11.8M 0s\n",
      "  9350K .......... .......... .......... .......... .......... 86% 22.9M 0s\n",
      "  9400K .......... .......... .......... .......... .......... 86% 20.2M 0s\n",
      "  9450K .......... .......... .......... .......... .......... 86% 16.1M 0s\n",
      "  9500K .......... .......... .......... .......... .......... 87% 10.4M 0s\n",
      "  9550K .......... .......... .......... .......... .......... 87% 25.6M 0s\n",
      "  9600K .......... .......... .......... .......... .......... 88% 8.45M 0s\n",
      "  9650K .......... .......... .......... .......... .......... 88% 11.2M 0s\n",
      "  9700K .......... .......... .......... .......... .......... 89% 25.6M 0s\n",
      "  9750K .......... .......... .......... .......... .......... 89% 31.4M 0s\n",
      "  9800K .......... .......... .......... .......... .......... 90% 9.51M 0s\n",
      "  9850K .......... .......... .......... .......... .......... 90% 9.15M 0s\n",
      "  9900K .......... .......... .......... .......... .......... 91% 9.87M 0s\n",
      "  9950K .......... .......... .......... .......... .......... 91% 30.3M 0s\n",
      " 10000K .......... .......... .......... .......... .......... 92% 20.9M 0s\n",
      " 10050K .......... .......... .......... .......... .......... 92% 23.2M 0s\n",
      " 10100K .......... .......... .......... .......... .......... 92% 11.3M 0s\n",
      " 10150K .......... .......... .......... .......... .......... 93% 16.4M 0s\n",
      " 10200K .......... .......... .......... .......... .......... 93% 21.5M 0s\n",
      " 10250K .......... .......... .......... .......... .......... 94% 12.9M 0s\n",
      " 10300K .......... .......... .......... .......... .......... 94% 7.33M 0s\n",
      " 10350K .......... .......... .......... .......... .......... 95% 28.4M 0s\n",
      " 10400K .......... .......... .......... .......... .......... 95% 21.0M 0s\n",
      " 10450K .......... .......... .......... .......... .......... 96% 17.9M 0s\n",
      " 10500K .......... .......... .......... .......... .......... 96% 3.41M 0s\n",
      " 10550K .......... .......... .......... .......... .......... 97% 19.6M 0s\n",
      " 10600K .......... .......... .......... .......... .......... 97% 18.6M 0s\n",
      " 10650K .......... .......... .......... .......... .......... 97% 11.2M 0s\n",
      " 10700K .......... .......... .......... .......... .......... 98% 7.78M 0s\n",
      " 10750K .......... .......... .......... .......... .......... 98% 22.8M 0s\n",
      " 10800K .......... .......... .......... .......... .......... 99% 13.9M 0s\n",
      " 10850K .......... .......... .......... .......... .......... 99% 12.2M 0s\n",
      " 10900K .......... .......... ..                              100% 13.2M=0.8s\n",
      "\n",
      "2022-10-10 22:37:53 (12.6 MB/s) - 'train.txt' saved [11185077/11185077]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-10-10 22:37:54--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/dev.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1385157 (1.3M) [text/plain]\n",
      "Saving to: 'dev.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  3% 5.13M 0s\n",
      "    50K .......... .......... .......... .......... ..........  7% 18.1M 0s\n",
      "   100K .......... .......... .......... .......... .......... 11% 14.2M 0s\n",
      "   150K .......... .......... .......... .......... .......... 14% 9.34M 0s\n",
      "   200K .......... .......... .......... .......... .......... 18% 16.3M 0s\n",
      "   250K .......... .......... .......... .......... .......... 22% 12.5M 0s\n",
      "   300K .......... .......... .......... .......... .......... 25% 19.5M 0s\n",
      "   350K .......... .......... .......... .......... .......... 29% 22.2M 0s\n",
      "   400K .......... .......... .......... .......... .......... 33% 18.2M 0s\n",
      "   450K .......... .......... .......... .......... .......... 36% 21.0M 0s\n",
      "   500K .......... .......... .......... .......... .......... 40% 21.1M 0s\n",
      "   550K .......... .......... .......... .......... .......... 44% 10.9M 0s\n",
      "   600K .......... .......... .......... .......... .......... 48% 21.7M 0s\n",
      "   650K .......... .......... .......... .......... .......... 51% 7.32M 0s\n",
      "   700K .......... .......... .......... .......... .......... 55% 20.6M 0s\n",
      "   750K .......... .......... .......... .......... .......... 59% 23.1M 0s\n",
      "   800K .......... .......... .......... .......... .......... 62% 13.4M 0s\n",
      "   850K .......... .......... .......... .......... .......... 66% 13.9M 0s\n",
      "   900K .......... .......... .......... .......... .......... 70% 11.6M 0s\n",
      "   950K .......... .......... .......... .......... .......... 73% 12.2M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 77% 21.2M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 81% 12.3M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 85% 38.4M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 88% 19.4M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 92% 14.9M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 96% 12.4M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 99% 55.8M 0s\n",
      "  1350K ..                                                    100% 4.84M=0.09s\n",
      "\n",
      "2022-10-10 22:37:54 (14.4 MB/s) - 'dev.txt' saved [1385157/1385157]\n",
      "\n",
      "--2022-10-10 22:37:54--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/test.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1398013 (1.3M) [text/plain]\n",
      "Saving to: 'test.txt'\n",
      "\n",
      "     0K .......... .......... .......... .......... ..........  3% 5.54M 0s\n",
      "    50K .......... .......... .......... .......... ..........  7% 62.1M 0s\n",
      "   100K .......... .......... .......... .......... .......... 10% 26.0M 0s\n",
      "   150K .......... .......... .......... .......... .......... 14% 17.0M 0s\n",
      "   200K .......... .......... .......... .......... .......... 18% 7.89M 0s\n",
      "   250K .......... .......... .......... .......... .......... 21% 17.1M 0s\n",
      "   300K .......... .......... .......... .......... .......... 25% 9.34M 0s\n",
      "   350K .......... .......... .......... .......... .......... 29% 8.65M 0s\n",
      "   400K .......... .......... .......... .......... .......... 32% 16.5M 0s\n",
      "   450K .......... .......... .......... .......... .......... 36% 18.3M 0s\n",
      "   500K .......... .......... .......... .......... .......... 40% 9.46M 0s\n",
      "   550K .......... .......... .......... .......... .......... 43% 20.8M 0s\n",
      "   600K .......... .......... .......... .......... .......... 47% 8.00M 0s\n",
      "   650K .......... .......... .......... .......... .......... 51% 15.3M 0s\n",
      "   700K .......... .......... .......... .......... .......... 54% 18.1M 0s\n",
      "   750K .......... .......... .......... .......... .......... 58% 23.8M 0s\n",
      "   800K .......... .......... .......... .......... .......... 62% 8.75M 0s\n",
      "   850K .......... .......... .......... .......... .......... 65% 24.0M 0s\n",
      "   900K .......... .......... .......... .......... .......... 69% 21.0M 0s\n",
      "   950K .......... .......... .......... .......... .......... 73% 27.0M 0s\n",
      "  1000K .......... .......... .......... .......... .......... 76% 9.62M 0s\n",
      "  1050K .......... .......... .......... .......... .......... 80% 27.7M 0s\n",
      "  1100K .......... .......... .......... .......... .......... 84% 10.5M 0s\n",
      "  1150K .......... .......... .......... .......... .......... 87% 15.7M 0s\n",
      "  1200K .......... .......... .......... .......... .......... 91% 17.2M 0s\n",
      "  1250K .......... .......... .......... .......... .......... 95% 16.6M 0s\n",
      "  1300K .......... .......... .......... .......... .......... 98% 11.1M 0s\n",
      "  1350K .......... .....                                      100% 93.5M=0.1s\n",
      "\n",
      "2022-10-10 22:37:55 (13.6 MB/s) - 'test.txt' saved [1398013/1398013]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O train.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/train.txt\n",
    "!wget -O dev.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/dev.txt\n",
    "!wget -O test.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ElrINWW7oF7"
   },
   "source": [
    "### 1.1 Building vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eawcuVV19kZm"
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "q-rNT_QL8Dvt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 20663\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "SOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "        data = [l.strip() for l in f.readlines()]\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_sentence_tokens(sentences):\n",
    "    return ['{} {} {}'.format(SOS, s, EOS) for s in sentences]\n",
    "\n",
    "\n",
    "def replace_oov(tokens):\n",
    "    vocab = nltk.FreqDist(tokens)\n",
    "    return [token if vocab[token] >= 3 else UNK for token in tokens]\n",
    "\n",
    "\n",
    "def pre_process(sentences):\n",
    "    sentences = add_sentence_tokens(sentences)\n",
    "    tokens = ' '.join(sentences).split(' ')\n",
    "    tokens = [token.lower() for token in tokens]\n",
    "    tokens = replace_oov(tokens)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "train = load_data(\"train.txt\")\n",
    "dev = load_data(\"dev.txt\")\n",
    "test = load_data(\"test.txt\")\n",
    "tokens = pre_process(train)\n",
    "vocab = nltk.FreqDist(tokens)\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocabulary size: {}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e7oBATsX8uHb"
   },
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PU6VpAkS9odh"
   },
   "source": [
    " Vocabulary size: 20663<br/>\n",
    " the number of parameters of n-gram model:$$20663^n$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJ2BGUig8TqH"
   },
   "source": [
    "### 1.2 $n$-gram Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeyANMPe9ad_"
   },
   "source": [
    "#### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "ACSfNZGE8Yw2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of unigram model on training set: 93.99585636227933\n",
      "Perplexity of unigram model on dev set: 75.43207223220776\n",
      "Perplexity of bigram model on training set: 17.29268572614725\n",
      "Perplexity of bigram model on dev set: 16.78284067451732\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "import math\n",
    "\n",
    "\n",
    "def build_multi_gram(n):\n",
    "    n_grams = nltk.ngrams(tokens, n)\n",
    "    n_vocab = nltk.FreqDist(n_grams)\n",
    "    if n < 2:\n",
    "        return {n_gram: count/len(tokens) for n_gram, count in n_vocab.items()}\n",
    "    m_grams = nltk.ngrams(tokens, n - 1)\n",
    "    m_vocab = nltk.FreqDist(m_grams)\n",
    "    return {n_gram: count/m_vocab[n_gram[:-1]] for n_gram, count in n_vocab.items()}\n",
    "\n",
    "\n",
    "def convert_oov(model, ngram, n):\n",
    "    masks = list(reversed(list(product((0, 1), repeat=n))))\n",
    "    mask = lambda ngram, bitmask: tuple((token if flag == 1 else \"<UNK>\" for token,flag in zip(ngram, bitmask)))\n",
    "    ngram = (ngram,) if type(ngram) is str else ngram\n",
    "    for possible_known in [mask(ngram, bitmask) for bitmask in masks]:\n",
    "        if possible_known in model:\n",
    "            return possible_known\n",
    "\n",
    "\n",
    "def cal_perplexity(test, n, model):\n",
    "    test_tokens = pre_process(test)\n",
    "    test_ngrams = nltk.ngrams(test_tokens, n)\n",
    "    M = len(test_tokens)\n",
    "    known_ngrams = (convert_oov(model, ngram, n) for ngram in test_ngrams)\n",
    "    probs = [model[ngram] for ngram in known_ngrams]\n",
    "    perplex = pow(2, (-1/M)*sum(map(math.log, probs)))\n",
    "    return perplex\n",
    "\n",
    "\n",
    "unigram_m = build_multi_gram(1)\n",
    "bigram_m = build_multi_gram(2)\n",
    "\n",
    "perplex_train_1 = cal_perplexity(train, 1, unigram_m)\n",
    "perplex_dev_1 = cal_perplexity(dev, 1, unigram_m)\n",
    "perplex_train_2 = cal_perplexity(train, 2, bigram_m)\n",
    "perplex_dev_2 = cal_perplexity(dev, 2, bigram_m)\n",
    "print(\"Perplexity of unigram model on training set: {}\\n\"\n",
    "      \"Perplexity of unigram model on dev set: {}\\n\"\n",
    "      \"Perplexity of bigram model on training set: {}\\n\"\n",
    "      \"Perplexity of bigram model on dev set: {}\".format(perplex_train_1, perplex_dev_1, perplex_train_2,\n",
    "                                                               perplex_dev_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRWC56a19TbY"
   },
   "source": [
    "#### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sM4gcgL--Ylh"
   },
   "source": [
    "Perplexity of unigram model on training set: 93.99585636227933<br/>\n",
    "Perplexity of unigram model on dev set: 75.43207223220776<br/>\n",
    "Perplexity of bigram model on training set: 17.29268572614725<br/>\n",
    "Perplexity of bigram model on dev set: 16.78284067451732<br/>\n",
    "\n",
    "Perplexity of unigram model on training set is larger than that on dev set, because UNK is used to replace words in test set which are not contained in training vocabulory, and the probility of UNK is large.<br/>\n",
    "Perplexity of bigram model on training set is similar with that on dev set, because even some words are replaced by UNK, P(Xi,UNK) is not that large compared with P(UNK) in unigram model.<br/>\n",
    "Perplexity of unigram model is larger than that of bigram model, because bigram model has context information and it can predict more precisely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuLL8CH1Ua-3"
   },
   "source": [
    "### 1.3 Smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7mWQhaCUixZ"
   },
   "source": [
    "#### 1.3.1 Add-one (Laplace) smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbbHxLDmVrz6"
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "93_yLu9dVr0C"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of bigram model on training set after smoothing: 86.515018275416\n",
      "Perplexity of bigram model on dev set after smoothing: 63.2280700395138\n"
     ]
    }
   ],
   "source": [
    "def laplace(k):\n",
    "    grams_2 = nltk.ngrams(tokens, 2)\n",
    "    vocab_2 = nltk.FreqDist(grams_2)\n",
    "\n",
    "    grams_1 = nltk.ngrams(tokens, 1)\n",
    "    vocab_1 = nltk.FreqDist(grams_1)\n",
    "    return {gram_2: (count + k) / (vocab_1[gram_2[:-1]] + vocab_size * k) for gram_2, count in vocab_2.items()}\n",
    "\n",
    "bigram_m = laplace(1)\n",
    "\n",
    "perplex_train = cal_perplexity(train, 2, bigram_m)\n",
    "perplex_dev = cal_perplexity(dev, 2, bigram_m)\n",
    "print(\"Perplexity of bigram model on training set after smoothing: {}\\n\"\n",
    "      \"Perplexity of bigram model on dev set after smoothing: {}\".format(perplex_train, perplex_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8y1WOQtsVr0D"
   },
   "source": [
    "##### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTknh9pRVr0D"
   },
   "source": [
    "Perplexity of bigram model on training set after smoothing: 86.515018275416<br/>\n",
    "Perplexity of bigram model on dev set after smoothing: 63.2280700395138<br/>\n",
    "\n",
    "Perplexity on training set and dev set both increase, because according to:\n",
    "$$P(Ci-1|Ci)=(P(Ci-1,Ci)+1)/(P(Ci-1)+v)$$\n",
    "the denominator of the probability increases vocab_size times faster than the numerator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pTC0qJE8VVha"
   },
   "source": [
    "##### Optional: Add-k smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3itGMOOVuNg"
   },
   "source": [
    "###### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "jhcuJWo7VuNg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When k=0.01:\n",
      "Perplexity of bigram model on training set: 23.570739594539667\n",
      "Perplexity of bigram model on dev set: 20.171703988957148\n",
      "When k=0.05:\n",
      "Perplexity of bigram model on training set: 32.73908329303373\n",
      "Perplexity of bigram model on dev set: 26.07385239496752\n",
      "When k=0.5:\n",
      "Perplexity of bigram model on training set: 67.07190203797246\n",
      "Perplexity of bigram model on dev set: 49.29044098488054\n"
     ]
    }
   ],
   "source": [
    "list_k = [0.01, 0.05, 0.5]\n",
    "for k in list_k:\n",
    "    bigram_m = laplace(k)\n",
    "    perplex_train = cal_perplexity(train, 2, bigram_m)\n",
    "    perplex_dev = cal_perplexity(dev, 2, bigram_m)\n",
    "    print(\"When k={}:\\n\"\n",
    "          \"Perplexity of bigram model on training set: {}\\n\"\n",
    "          \"Perplexity of bigram model on dev set: {}\".format(k, perplex_train, perplex_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzpU_p6CVuNg"
   },
   "source": [
    "###### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIOUpNXYVuNh"
   },
   "source": [
    "When k=0.01:<br/>\n",
    "Perplexity of bigram model on training set: 23.570739594539667<br/>\n",
    "Perplexity of bigram model on dev set: 20.171703988957148<br/>\n",
    "When k=0.05:<br/>\n",
    "Perplexity of bigram model on training set: 32.73908329303373<br/>\n",
    "Perplexity of bigram model on dev set: 26.07385239496752<br/>\n",
    "When k=0.5:<br/>\n",
    "Perplexity of bigram model on training set: 67.07190203797246<br/>\n",
    "Perplexity of bigram model on dev set: 49.29044098488054<br/>\n",
    "\n",
    "Larger k leading to larger perplexity, because according to:\n",
    "$$P(Ci-1|Ci)=(P(Ci-1,Ci)+k)/(P(Ci-1)+k*v)$$\n",
    "when k increase, the denominator of the probability linearly increases v times. Larger k leads to smaller probility of sentences in train sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJFWxsN-Uj0Y"
   },
   "source": [
    "#### 1.3.2 Linear Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk11EpboWVCH"
   },
   "source": [
    "##### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "K4N_XuN6WVCQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperparameter set:0.0, 0.0, 1.0:\n",
      "Perplexity on training set:93.99585636227933\n",
      "Perplexity on dev set:75.43207223220776\n",
      "\n",
      "hyperparameter set:0.0, 0.2, 0.8:\n",
      "Perplexity on training set:35.32091550499629\n",
      "Perplexity on dev set:28.918593160262922\n",
      "\n",
      "hyperparameter set:0.0, 0.4, 0.6:\n",
      "Perplexity on training set:26.490784435938586\n",
      "Perplexity on dev set:23.13301243763304\n",
      "\n",
      "hyperparameter set:0.0, 0.6, 0.4:\n",
      "Perplexity on training set:22.00629511068622\n",
      "Perplexity on dev set:20.027125913975368\n",
      "\n",
      "hyperparameter set:0.0, 0.8, 0.19999999999999996:\n",
      "Perplexity on training set:19.197558177553166\n",
      "Perplexity on dev set:18.05440150900059\n",
      "\n",
      "hyperparameter set:0.0, 1.0, 0.0:\n",
      "Perplexity on training set:17.29268572614725\n",
      "Perplexity on dev set:16.78284067451732\n",
      "\n",
      "hyperparameter set:0.2, 0.0, 0.8:\n",
      "Perplexity on training set:11.272936768296475\n",
      "Perplexity on dev set:13.931227143669025\n",
      "\n",
      "hyperparameter set:0.2, 0.2, 0.6:\n",
      "Perplexity on training set:9.795959658306847\n",
      "Perplexity on dev set:11.87727735869802\n",
      "\n",
      "hyperparameter set:0.2, 0.4, 0.4:\n",
      "Perplexity on training set:8.876991117350785\n",
      "Perplexity on dev set:10.632037724928866\n",
      "\n",
      "hyperparameter set:0.2, 0.6, 0.19999999999999996:\n",
      "Perplexity on training set:8.218087474392652\n",
      "Perplexity on dev set:9.765218374012765\n",
      "\n",
      "hyperparameter set:0.2, 0.8, 0.0:\n",
      "Perplexity on training set:7.716635830408419\n",
      "Perplexity on dev set:9.133240366823275\n",
      "\n",
      "hyperparameter set:0.4, 0.0, 0.6:\n",
      "Perplexity on training set:7.600285809379266\n",
      "Perplexity on dev set:10.746637237495188\n",
      "\n",
      "hyperparameter set:0.4, 0.2, 0.4:\n",
      "Perplexity on training set:6.967077906433637\n",
      "Perplexity on dev set:9.653565960021863\n",
      "\n",
      "hyperparameter set:0.4, 0.4, 0.19999999999999996:\n",
      "Perplexity on training set:6.5114292979184425\n",
      "Perplexity on dev set:8.90094240959446\n",
      "\n",
      "hyperparameter set:0.4, 0.6, 0.0:\n",
      "Perplexity on training set:6.161131486374682\n",
      "Perplexity on dev set:8.351189624376735\n",
      "\n",
      "hyperparameter set:0.6, 0.0, 0.4:\n",
      "Perplexity on training set:5.966340403214723\n",
      "Perplexity on dev set:9.078342960415787\n",
      "\n",
      "hyperparameter set:0.6, 0.2, 0.19999999999999996:\n",
      "Perplexity on training set:5.599188788368883\n",
      "Perplexity on dev set:8.366000999571012\n",
      "\n",
      "hyperparameter set:0.6, 0.4, 0.0:\n",
      "Perplexity on training set:5.318568703279823\n",
      "Perplexity on dev set:7.855933261992395\n",
      "\n",
      "hyperparameter set:0.8, 0.0, 0.19999999999999996:\n",
      "Perplexity on training set:5.00628493611841\n",
      "Perplexity on dev set:8.01876349744224\n",
      "\n",
      "hyperparameter set:0.8, 0.2, 0.0:\n",
      "Perplexity on training set:4.764036639382034\n",
      "Perplexity on dev set:7.516673035350014\n",
      "\n",
      "hyperparameter set:1.0, 0.0, 0.0:\n",
      "Perplexity on training set:4.365120217349759\n",
      "Perplexity on dev set:7.297414670618797\n",
      "\n",
      "best hyperparameter set:1.0, 0.0, 0.0\n",
      "Perplexity on test set:7.296406756886781\n"
     ]
    }
   ],
   "source": [
    "def linear_interpolation(k_list, vocab_l):\n",
    "    def p_3(gram_3, count):\n",
    "        n_2 = vocab_l[1][gram_3[:-1]]\n",
    "        if n_2 == 0:\n",
    "            return 0\n",
    "        return count/n_2\n",
    "\n",
    "    def p_2(gram_3):\n",
    "        n_1 = vocab_l[0][gram_3[1:-1]]\n",
    "        if n_1 == 0:\n",
    "            return 0\n",
    "        return vocab_l[1][gram_3[1:]]/n_1\n",
    "\n",
    "    def p_1(gram_3):\n",
    "        t = vocab_l[0][gram_3[2:]] / len(tokens)\n",
    "        return t\n",
    "\n",
    "    return {gram_3: k_list[0] * p_3(gram_3, count) + k_list[1] * p_2(gram_3) + k_list[2] * p_1(gram_3)\n",
    "            for gram_3, count in vocab_l[2].items()}\n",
    "\n",
    "\n",
    "def cal_linear_perplex(test, k_list, model):\n",
    "    n = 3\n",
    "    if k_list[0] == 0:\n",
    "        n = 2\n",
    "        if k_list[1] == 0:\n",
    "            n = 1\n",
    "    model_n = {gram3[3-n:]: count for gram3, count in model.items()}\n",
    "    return cal_perplexity(test, n, model_n)\n",
    "\n",
    "\n",
    "def search_linear_param():\n",
    "    grams_3 = nltk.ngrams(tokens, 3)\n",
    "    vocab_3 = nltk.FreqDist(grams_3)\n",
    "\n",
    "    grams_2 = nltk.ngrams(tokens, 2)\n",
    "    vocab_2 = nltk.FreqDist(grams_2)\n",
    "\n",
    "    grams_1 = nltk.ngrams(tokens, 1)\n",
    "    vocab_1 = nltk.FreqDist(grams_1)\n",
    "\n",
    "    vocab_l = [vocab_1, vocab_2, vocab_3]\n",
    "    min = 10000000\n",
    "    for k_1 in range(0, 6):\n",
    "        for k_2 in range(0, 6 - k_1):\n",
    "            k_list = [k_1/5, k_2/5, 1-(k_1 + k_2)/5]\n",
    "            linear_m = linear_interpolation(k_list, vocab_l)\n",
    "            perplex_dev = cal_linear_perplex(dev, k_list, linear_m)\n",
    "            perplex_train = cal_linear_perplex(train, k_list, linear_m)\n",
    "            print(\"hyperparameter set:{}, {}, {}:\\n\"\n",
    "                  \"Perplexity on training set:{}\\n\"\n",
    "                  \"Perplexity on dev set:{}\\n\".format(k_list[0], k_list[1], k_list[2], perplex_train, perplex_dev)\n",
    "                  )\n",
    "            if perplex_dev < min:\n",
    "                min = perplex_dev\n",
    "                best_param = k_list\n",
    "    perplex_test = cal_linear_perplex(test, k_list, linear_m)\n",
    "    print(\"best hyperparameter set:{}, {}, {}\\n\"\n",
    "          \"Perplexity on test set:{}\".format(best_param[0], best_param[1], best_param[2], perplex_test))\n",
    "\n",
    "\n",
    "search_linear_param()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kh8v2P36WVCQ"
   },
   "source": [
    "##### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGkf6IV0WVCQ"
   },
   "source": [
    "best hyperparameter set:1.0, 0.0, 0.0<br/>\n",
    "Perplexity on training set:4.365120217349759<br/>\n",
    "Perplexity on dev set:7.297414670618797<br/>\n",
    "Perplexity on test set:7.296406756886781<br/>\n",
    "\n",
    "because in this case, vocabulary is build from train set and in the training process zero-probility is not existed. There is no need to use linear interpolation and it is better to use tigram model which is precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wvgTZcNFVato"
   },
   "source": [
    "##### Optional: Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKnXu98hWcfu"
   },
   "source": [
    "###### Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKo4ZLASWcfu"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3E8xYXuCUoAR"
   },
   "source": [
    "## 2 Preposition Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "gewisxM5W5kQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-10-10 23:06:42--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.in\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 232043 (227K) [text/plain]\n",
      "Saving to: 'dev.in'\n",
      "\n",
      "     0K .......... .......... .......... .......... .......... 22% 2.38M 0s\n",
      "    50K .......... .......... .......... .......... .......... 44% 29.8M 0s\n",
      "   100K .......... .......... .......... .......... .......... 66% 27.5M 0s\n",
      "   150K .......... .......... .......... .......... .......... 88% 31.8M 0s\n",
      "   200K .......... .......... ......                          100% 38.3M=0.03s\n",
      "\n",
      "2022-10-10 23:06:43 (8.46 MB/s) - 'dev.in' saved [232043/232043]\n",
      "\n",
      "--2022-10-10 23:06:43--  https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.out\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 8625 (8.4K) [text/plain]\n",
      "Saving to: 'dev.out'\n",
      "\n",
      "     0K ........                                              100%  520K=0.02s\n",
      "\n",
      "2022-10-10 23:06:43 (520 KB/s) - 'dev.out' saved [8625/8625]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget -O dev.in https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.in\n",
    "!wget -O dev.out https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7184889211768979\n"
     ]
    }
   ],
   "source": [
    "PREP = \"<PREP>\"\n",
    "\n",
    "\n",
    "def pre_process2(sentences):\n",
    "    sentences = add_sentence_tokens(sentences)\n",
    "    tokens = ' '.join(sentences).split(' ')\n",
    "\n",
    "    tokens = [token.lower() if token != PREP else token for token in tokens]\n",
    "\n",
    "    sentences_pred = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == PREP:\n",
    "            sentences_pred.append(tokens[i-1:i+2])\n",
    "    return sentences_pred\n",
    "\n",
    "\n",
    "def cal_sentence_perplexity(sentence, prep):\n",
    "    sentence = [prep if w == PREP else w for w in sentence]\n",
    "    grams = nltk.ngrams(sentence, 2)\n",
    "    probs = [bigram_m_2[gram] if gram in bigram_m_2.keys() else math.exp(-100) for gram in grams]\n",
    "    probs.sort()\n",
    "    perplex = pow(2, -1*sum(map(math.log, probs)))\n",
    "    return perplex\n",
    "\n",
    "\n",
    "def pred_prep(sentence):\n",
    "    prep_list = [\"at\", \"in\", \"of\", \"for\", \"on\"]\n",
    "    perplex_list = {prep: cal_sentence_perplexity(sentence, prep) for prep in prep_list}\n",
    "    prep = min(perplex_list.keys(), key=(lambda k: perplex_list[k]))\n",
    "    return prep\n",
    "\n",
    "\n",
    "def cal_accuracy():\n",
    "    actual_res = ' '.join(dev_out).split(' ')\n",
    "    true_count = 0\n",
    "    for i, res in enumerate(actual_res):\n",
    "        if pred_res[i] == res:\n",
    "            true_count = true_count + 1\n",
    "    return true_count/len(actual_res)\n",
    "\n",
    "\n",
    "def output_format():\n",
    "    format = [line.count(PREP) for line in test_in]\n",
    "    f = open(\"3036033675.test.out\", \"a+\")\n",
    "    start = 0\n",
    "    for count in format:\n",
    "        for i in range(0, count):\n",
    "            if i == count - 1:\n",
    "                f.write(\"{}\\n\".format(test_res[start]))\n",
    "            else:\n",
    "                f.write(\"{} \".format(test_res[start]))\n",
    "            start = start + 1\n",
    "    f.close()\n",
    "\n",
    "\n",
    "bigram_m_2 = build_multi_gram(2)\n",
    "dev_in = load_data(\"dev.in\")\n",
    "dev_out = load_data(\"dev.out\")\n",
    "test_in = load_data(\"test.in\")\n",
    "sentences = pre_process2(dev_in)\n",
    "pred_res = [pred_prep(sentence) for sentence in sentences]\n",
    "accuracy = cal_accuracy()\n",
    "print(accuracy)\n",
    "\n",
    "sentences = pre_process2(test_in)\n",
    "test_res = [pred_prep(sentence) for sentence in sentences]\n",
    "output_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKo4ZLASWcfu"
   },
   "source": [
    "I tried trigram model and bigram model to do preposition prediction, and the result is that, the accuracy of bigram model on dev set is 71.8%, which is a lot larger than that of trigram model. Because the vocabulary and training set are not large encough, useing trigram model will lead to many UNK. Besides, when caculating perplexity, at first, I use UNK to convert grams not seen in traing corpus, to deal with zero-probility. But this method turned out to have low-accuracy. Then I add e-100 to solve zero-probility problem. <br/> \n",
    "Theoretically, model with high perplexity on traing will perform better in application. But in this experiment, trigram model with higher perlexity performs worse. Because the model just fit training set which is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
